# 190917Stat by 재명
## 예측
### 회귀분석
* 잔차
  - 회귀 분석의 예측과 실제값의 차이 / 틀린 것!
  - 잔차의 분포에서 주목하는 특성들 - 왜도, 첨도, 등분산성
* 왜도 (Skewness) / 데이터의 분포에 대해 치우친 정도
  - negative Skew : 오른쪽으로 치우짐
  - Positive Skew : 왼쪽으로 치우침 (ex. 소득)
  - <b>잔차가 Skewness되어있다는 것은? 데이터가 치우쳐있을 가능성이 높음</b>
* 첨도 (Kurtosis)
  - 분포가 한 점에 몰린 정도
  - 정규분포의 첨도 = 3
  - 첨도가 높다 -> 데이터가 중심에 몰려 있음
  - 첨도가 낮다 -> 데이터가 바깥으로 퍼져 있음
  - 첨도가 낮은 거는 상관없는데 높은 것은 문제가 있을 수 있음 -> 문제가 있을 거 같은 경우에는 데이터를 확인해보기
* 등분산성 (Homoscedasticity)
  - 모든 범위에서 잔차의 분산이 같음 -> 잔차가 벗어나는 정도인데 모든 범위에서 벗어나는 정도가 같다는 것? 예측하기 쉽다는 것
  - 쉽게 말해 어떤 x에서든 비슷한 정도로 y를 맞출 수 있음
  - Dubin-Watson 통계량이 1~2 정도 -> 적당함
* 잔차의 정규성
  - 잔차가 정규분포에 가까운 성질을 가지고 있는가? -> 정규분포에 가깝길 원함. 왜?
  - Omnibus / Jarque-Bera
  - 둘 다 Prob가 1에 가까울 수록 정규분포에 가까움
* 조건수 (Condition number)
  - 입력의 변화에 따른 출력의 변화를 나타내는 수 
  - 조건수가 크면 -> 데이터가 조금만 달라져도 결과에 큰 차이
  - 조건수가 크면 안 좋음
  - 통상 30 이하
  - <b>다중공선성(Multicollinearity)</b>
    - 독립변수들이 서로 예측 가능할 경우 조건수가 커짐
    - 데이터나 변수의 변화에 따라 추정된 계수가 크게 달라짐
    - 다중공산성이 있는 경우 MSE가 1개이상일 수 있음
    - Condition Number는 꼭 확인해야함
    - [2] The smallest eigenvalue is 1.21e-30. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. / 다중공산성이 있는 경우 이와 같은 메세지가 뜸
* 정리
  - 다중공선성(독립변수끼리 예측이 되는 경우) -> 무수히 많은 계수가 가능 -> 계수 추정이 불안정해짐 -> 조건수가 높아짐
* 교차 검증
  - 데이터를 무작위로 두 세트로 나눔
  - 한 세트에서 추정 -> 다른 세트에서 검증
  - 위 과정을 반복
* k-fold Cross Validation
  - 데이터를 k개로 나누어 CV를 k번 하는 방법
* 정규화 (Regularization)
  - <b>과적합(overfitting):모형의 계수가 주어진 데이터(sample)에 지나치게 의존하여 추정되는 경우</b>
  - 과적합이 되면 우리가 갖고 있는 샘플에만 잘 맞고 새로운 샘플에 잘 맞지 않음
  - 변수가 많을 수록, 계수가 클 수록 과적합의 위험이 큼
  - 만약 계수가 0이라면 변수를 추가하지 않은 것과 같음
  - 정규화 : 가능한 계수를 작게 추정하는 방법 -> 과적합을 막아 줌 (예측을 잘 함) / 계수가 0이 되면 변수를 빼는 것과 같은 효과를 줌 (부가적인 효과)
* 변수 선택
  - 변수를 빼거나, 빼지 않거나 경우의 수를 조합해보아야 한다. -> 조합이 너무 많아서 물리적으로 불가능함
  - 변수를 빼는 것이 불가능하므로 변수의 영향력을 줄이기 위해 계수가 0에 가까워지도록 함
* 회귀분석에서의 정규화
  - 보통 회귀분석은 y의 예측에 대한 오차 (MSE)를 최소화
  - (오차 + 계수)를 최소화하도록 함
  - 오차가 조금 늘어나더라도 계수를 작게 만들어 과적합을 방지
* 라쏘 회귀분석
  - 계수의 절대값을 최소화
  - 람다 : 클 수록 계수를 최소화하는 데 더 큰 비중
  - 회귀계수를 0으로 만드는 경향 -> 변수 선택
* 릿지 회귀분석
  - 계수의 절대값을 최소화
  - 라쏘보다 대체로 나음. 회귀계수가 0이 되지는 않음
  - 딥러닝에서 '가중치 감쇠(weight decay)'라고 부름
  - 그렇다면 라쏘는 왜 사용함? 계수를 0으로 만들어줘서 해석이 심플해짐
  - 결론은 MSE를 줄이느냐, MSE랑 계수를 같이 줄이느냐? 하는 문제
* 엘라스틱 넷
  - 라쏘 + 릿지
  - a = 1:라쏘, a = 0; 릿지
* 하이퍼파라미터
  - 모형의 특성을 결정하지만 데이터로부터 학습되지 않는 값
  - 엘라스틱 넷에서 람다와 a, CV를 통해 결졍
  - 가장 예측이 잘 되는 람다와 알파를 찾아 하이퍼 파라미터를 사용함
* 과적합 X
  - 설명을 못 하게 한다. 
  - 변수를 줄인다
  - 계수를 줄인다 -> 정규화
  - 곡선X, 직선O

### 회귀분석 고급
* 더미 코딩(dummy coding)
  - 독립변수에 이산형(범주형) 변수가 있을 경우 기준이 되는 값을 정함
  - 나머지 값을 새로운 변수로 추가
  - 해당되는 변수의 값을 1, 나머지는 0으로 설정
  - 변수가 n개면 카테고리는 n-1개
  - 가나다 순으로 기준이 생김
* 상호작용 (interaction)
  - 두 독립변수의 곱으로 이뤄진 항 (y=x+m+xm)
  - 상호작용이 없는 경우 m에 따라 x의 절편이 영향을 받음
  - 상호작용이 있는 경우 m에 따라 (1) 기울기가 영향을 받거나 (2) 기울기와 절편이 둘 다 영향을 받음
  - 영향을 끼치는 정도에 영향을 미침
  - 상호작용이 있는 경우 (1) y=x+xm (2) y=x+m+xm
  - 상호작용을 넣을지 말지는 분석가의 판단
  