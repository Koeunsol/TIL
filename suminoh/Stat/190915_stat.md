# 190915_Stat by 재명

## 예측

### <b>상관</b>
* 산점도 
  - 두 연속 변수의 관계를 시각화
  - 한 건의 데이터를 점으로 표시
  - 산점도를 그리는 목적은 두 변수의 관계를 눈으로 확인하는데 있음
  - 엑셀에서는 분산형 차트를 그리면 됨
* 공분산
  - 함께 퍼진 정도
  - 두 연속 변수의 관계를 수치화
  - 두 변수가 같은 방향으로 변하면 +, 반대 방향으로 변하면 - 부호를 지님
  - 쉽게 말해서 비례, 역비례와 비슷
  - 함께 변하는 경향이 강할 수록 절대적 크기가 커짐
* 상관계수
  - 공분산을 두 변수의 표준편차로 나눈 것
  - 항상 -1 ~ +1 의 범위
  - +1 : 완벽하게 같은 방향으로 움직임
  - 0 : 아무 관계 없음
  - -1 : 완벽하게 반대 방향으로 움직임 
  - 데이터를 많이 보고 수치를 파악할 수 있는 감이 생겨야함
  - 기울기랑은 관련이 없고 얼마나 완벽하게 직선관계를 보이냐에 따라 값이 결정됨
  - 직선이 아닌, U, W...과 같은 패턴은 상관계수를 알 수 없으므로 꼭 산점도를 그려서 확인해봐야 함
  - 상관계수가 0이라고 데이터의 패턴이 없는 것이 아님
* 허위 상관관계
  - 두 변수 사이에 실제로는 관계가 없어도 상관관계가 나타나는 경우
  - 데이터가 적을 수록 나타나기 쉽다
  - 상관계수의 신뢰구간을 확인 / 부호가 변하는 신뢰구간은 데이터를 더 모아서 확인
* 여러가지 상관계수
  - 피어슨(Pearson) 상관계수
    - 일반적으로 말하는 상관계수가 이것임
  - 스피어만(Spearman) 상관계수
    - 실제 변수값 대신 그 서열을 사용하여 피어슨 사오간계수를 계산
    - 한 변수의 서열이 높아지면 다른 변수의 서열도 높아지는 지를 관찰함
  - 켄달(Kendall) 상관계수
    - 스피어만 상관계수와 비슷하게 서열의 관계를 수치화, 계산 방법이 다름
    - 데이터가 작고, 동점이 많을 때 사용
  - 피어슨은 선형이기 떄문에 상관관계까 안 나오도 스피어만, 켄달에서만 나타날 수 있음 
    - 스피어만, 켄달에서 값이 다르면 데이터를 더 모아봐야함
* P-value 
  - p값 < .05 : 95% 신뢰구간 반대 부호가 포함 X
  - p값 > .01 : 99% 신뢰구간 반대 부호가 포함 X
  - p값 < .001 : 99.9% 신뢰구간 반대 부호가 포함 X
  - p-value의 값은 작을 수록 좋음
### <b>회귀분석</b>
* 회귀분석 
  - 여러 가지 의미로 사용
  - 넓은 의미 : X -> Y 예측
  - 중간 의미 : Y가 연속인 경우(Y가 범주형인 경우는 분류)
  - 좁은 의미 : 선형 회귀 분석(선형 모형을 이용한 회귀분석)
  - 회귀분석? 회귀선, 추세선 / 추세선으로 회귀함, 돌아감
* 절편과 계수
  - 절편 : 독립변수가 모두 0일때 종속변수의 값
  - 계수 : 독립변수가 1 증가할 때 종속변수의 변화
* 계수의 신뢰구간
* R제곱
  - 독립변수가 많을 수록 높아지는 경향이 있음
  - 수정 R제곱 : 독립변수의 개수를 보정
* 모형선택 -> 변수해석
  - 모형을 선택할 때는 모형 전체의 설명도를 비교하고, 그 후에 변수들을 확인
  - 변수를 삭제하고 다시 모형을 비교하여 설명도가 떨어진다면 삭제하지 말아야 함
  - R-squared는 단순 설명도, Adj.R-squared는 모형 비교
  - 비교는 Adj.R-squared, AIC, BIC 세 가지를 비교
* 통계 vs 머신러닝
  - 통계에선 모형선택 후 변수 해석 / 예측과, 그 이유를 알려고 함
  - 머신러닝에서는 변수해석을 하지 않음 / 예측만 잘 하면 됨

* reg.summary()해석
  - coef : 계수
  - [0.025, 0.975] : 95% 신뢰구간 -> 부호가 바뀌지 않으니 믿어도 됨  
  - P>|t| : P-value  
  - std err : 신뢰구간을 계산하는 이론적 수치 / 직접 해석할 일 없음
  - t : P>|t|를 계산하는 이론적 수치 / 직접 해석할 일 없음  
  - R-squared : 에타제곱 (분산 %) / 에타제곱은 두 집단 간의 차이를 말할 때 사용, 회귀에서는 R제곱을 사용 / 모형적합도 지수  
  -  Adj.R-squared : R제곱을 보정
  - R-squared는 독립변수와 종속변수의 상관계수를 제곱한 값과 같음
  - F-statistic : 절편을 제외한 모든 회귀계수가 0일 때를 가정하고 계산한 수치 / 독립변수를 다 0이여도 이런 결과가 나올 수 있는가?
  - Prob(F-statistic) : P-value랑 비슷하게 해석 / 0.05보다 크면 데이터를 더 보아야함
  - Log-Likelihood : 로그우도 / 모델을 만들었을 때, 현재 데이터가 관찰될 확률 / 0에 가까울 수록 좋음 / 독립변수가 많아지면 좋아지는 경향이 있음
  - AIC / BIC : 로그우도를 보정 / 낮을 수록 좋음  
  - 높을수록, 낮을수록 좋은 것의 기준은 서로 다른 두 모델의 적합도를 비교하는 것  
  1. 독립변수의 계수 -> 신뢰구간(부호 확인)
  2. Prob(F) < 0.05
  3. 모형 비교 Adj.R-squared는 높을수록 AIC, BIC는 낮을수록 좋음
* 단계적 회귀분석
  - 변수를 단계적으로 추가/삭제하는 방법
  - 과거에는 많이 사용했으나 근본적 한계가 있음
  - 데이터가 적고, 변수의 수도 적다면 쓸 수도 있음
  - 그렇지 않다면 더 좋은 방법이 있어서 단계적 회귀분석을 사용하지 않음
* 전방선택
  - 절편만 있는 모형으로 시작
  - 추가했을 때 모형을 가장 많이 개선할 수 있는 변수를 추가
  - 더 이상 모형이 개선되지 않으면 중단
  - 지나치게 적은 변수를 포함시킬 위험이 존재
* 후방선택
  - 모든 변수를 투입한 모형으로 시작
  - 제외했을 때 모형을 가장 많이 개선하는 변수를 제외
  - 더 이상 모형이 개선되지 않으면 중단
  - 지나치게 많은 변수를 포함시킬 위험이 존재
  