# 190902 Stat by 재명
## 복습
1. 분포(이항분포, 정규분포)와 파라미터  
* 분포와 파라미터 - 분포는 레시피, 분포의 특성을 조절하는 것이 파라미터  
* 우리가 배우는 분포들은 파라미터가 간단 / 계산이 비교적 쉬운 편  
* 계산하기 어려운 분포는 딥러닝  
* 딥러닝도 어떻게 보면 일종의 분포를 만드는 것  
* 반면 이항분포, 정규분포는 수식을 외워서 바로 사용할 수 있음  
* 이항, 정규가 핵심 분포로 사용됨 -> 계산이 쉬움!!  
2. 그래프 모형  
* 두 개의 변수의 영향를 나타낸 것  
3. 선형 모형 / 로지스틱 선형 모형  
* 어떻게 영향을 주는 지 디테일하게 보여주는 것  
* 선형모형 ; y = ax + b ; 직선의 방정식 ; x -> y  
* 식이 간단할수록 틀릴 가능성이 낮음, 단순한 것이 좋음!   
* 틀리는 것도 적게 틀리지만, 맞는 것도 적게 맞음^^;;;  
* 로지스틱 선형 모형은 선형모형의 결과를 커브에 집어넣음 -> <b>예측가능? 보충설명 필요</b>  

## 이항분포
* a를 할 때 b일 확률이 p이면 <b>a를 n번 했을 때 b가 x번 발생할 확률 분포</b>  
* n, p가 파라미터
* 무엇을 이항분포로 나타낼지가 중요
* ex) 동전을 5번 던졌을 때, 앞면이  3번 발생할 확률?  
a = 동전던지기, b = 앞면 나오기, p = 1/2, n = 5, x = 3
* ex) 고객이 100이 있을 때 방문할 확률은 30%이다. 그 때 20명의 고객이 방문할 확률?  
a = 고객, b = 고객 방문, p = 3/10, n = 100, x = 20  
* ex) 복권 추첨을 할 때, 당첨될 확률이 1/10이라면, 100번 복권을 살 때 20번 복권에 당첨될 확률?  

## 최적화를 이용한 파라미터 추정
* <b>최적화</b> : 제약조건 아래서 목표 함수를 최대화(최소화) 하는 것  
* <b>파라미터 추청</b> : 분포 / 모형의 파라미터를 추정하는 것  
* ex) 목표함수 J가 있을 떄, L = J(p) : 오차를 계산하는 함수 / 오차(parameter)
* 오차가 가장 작아지는 parameter를 찾는 것
* 오차를 어떻게 측정하냐에 따라 parameter가 달라질 수 있음
## 평균오차제곱 (Mean Squared Error)
* 연속변수를 예측할 때 사용
* 오차 = 예측과 실제의 차이
* y - yhat ; 실제값 - 예측값 = 오차
* MSE = (y-yhat)^2/n
* MAE; 절댓값을 사용하여 평균내는 것; 계산하는 것이 어려워져서 잘 사용하지 않음
* 평균오차제곱(MSE)이 최소화되도록 파라미터를 추정
* 모든 예측을 평균으로 하면 MSE는 분산과 같아짐
* 정규분포에서 나온 데이터가 있을 때 모분포의 평균을 어떻게 추정하나?  
MSE가 가장 작아지는 예측값을 모분포의 평균으로 추정  
(수학적인 이유로 데이터의 평균이 MSE가 가장 작음)  
(그래서 데이터의 평균이 모분포의 평균에 대한 가장 좋은 추정치가 됨)  
* minimize(오차추정함수, 초기값)을 사용하여 오차를 찾으면 됨
-> 경사하강법 
* 오차가 적은 parameter를 찾는 것은 데이터를 정확히 예측할 수 있는 그래프를 찾는 것
## 우도 (likelihood)
* 있을법함, 그럴듯함
* 모형과 파라미터를 가정했을 때 현재 데이터 D가 관찰될 확률  
* P(D|M)  
* 우도 - (파라미터를 가정했을때,) 데이터가 관찰될 확률
* 최대우도법(Maximum Likelihood):우도를 최대로 하는 파라미터를 찾는 방법
## 로그 우도
* 많은 우도를 곱할 경우(독립의 경우 곱해서 계산 p*q*r) 계산이 까다로움
* 우도 대신 로그 우도를 사용하면 곱셈 대신 덧셈을 사용할 수 있음
log(a*b) = log(a) + log(b)